#BlueJ class context
comment0.params=args
comment0.target=void\ main(java.lang.String[])
comment1.params=input\ hidden\ output\ i1\ i2\ i3\ o1
comment1.target=RM6Network(int,\ int,\ int,\ java.lang.String,\ java.lang.String,\ java.lang.String,\ java.lang.String)
comment2.params=
comment2.target=double\ getRandom()
comment3.params=inputs
comment3.target=void\ setInput(double[])
comment3.text=\r\n\ \r\n\ @param\ inputs\r\n\ \ \ \ \ \ \ \ \ \ \ \ There\ is\ equally\ many\ neurons\ in\ the\ input\ layer\ as\ there\ are\r\n\ \ \ \ \ \ \ \ \ \ \ \ in\ input\ variables\r\n
comment4.params=
comment4.target=double[]\ getOutput()
comment5.params=
comment5.target=void\ activate()
comment5.text=\r\n\ Calculate\ the\ output\ of\ the\ neural\ network\ based\ on\ the\ input\ The\ forward\r\n\ operation\r\n
comment6.params=expectedOutput
comment6.target=void\ applyBackpropagation(double[])
comment6.text=\r\n\ all\ output\ propagate\ back\r\n\ \r\n\ @param\ expectedOutput\r\n\ \ \ \ \ \ \ \ \ \ \ \ first\ calculate\ the\ partial\ derivative\ of\ the\ error\ with\r\n\ \ \ \ \ \ \ \ \ \ \ \ respect\ to\ each\ of\ the\ weight\ leading\ into\ the\ output\ neurons\r\n\ \ \ \ \ \ \ \ \ \ \ \ bias\ is\ also\ updated\ here\r\n
comment7.params=maxSteps\ minError
comment7.target=void\ run(int,\ double)
comment8.params=
comment8.target=void\ printResult()
comment9.params=
comment9.target=void\ printAllWeights()
numComments=10
